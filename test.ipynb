{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dac9b91-f2f3-4b43-b183-ac1b62ee876b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully retrieved page 1\n",
      "Extracted property 1: 3407 Amber Forest Dr, Houston, TX 77068\n",
      "Sleeping for 0.40 seconds...\n",
      "Extracted property 2: 1920 CREEK HOLW, San Antonio, TX 78259\n",
      "Sleeping for 0.32 seconds...\n",
      "Extracted property 3: 10799 Aaron St, El Paso, TX 79924\n",
      "Sleeping for 0.24 seconds...\n",
      "Extracted property 4: 2203 BLUEBERRY HILL ST, San Antonio, TX 78232\n",
      "Sleeping for 0.46 seconds...\n",
      "Extracted property 5: 6217 Decatur Ct, Frisco, TX 75035\n",
      "Sleeping for 0.46 seconds...\n",
      "Extracted property 6: 8783 TIMBER POINT ST, San Antonio, TX 78250\n",
      "Sleeping for 0.27 seconds...\n",
      "Extracted property 7: 8328 Meadow Sweet Ln, Fort Worth, TX 76123\n",
      "Sleeping for 0.36 seconds...\n",
      "Extracted property 8: 6313 Truman Dr, Fort Worth, TX 76112\n",
      "Sleeping for 0.35 seconds...\n",
      "Extracted property 9: 8442 Horsepen Bend Dr, Conroe, TX 77385\n",
      "Sleeping for 0.22 seconds...\n",
      "No next page found. Stopping scraping.\n",
      "Data successfully saved to 'zillow_properties_with_delays.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Define the base Zillow URL \n",
    "base_url = \"https://www.zillow.com/tx/?searchQueryState=%7B%22isMapVisible%22%3Afalse%2C%22mapBounds%22%3A%7B%22west%22%3A-106.844420625%2C%22east%22%3A-93.309264375%2C%22south%22%3A24.969911847575055%2C%22north%22%3A37.26862098795179%7D%2C%22regionSelection%22%3A%5B%7B%22regionId%22%3A54%2C%22regionType%22%3A2%7D%5D%2C%22filterState%22%3A%7B%22sort%22%3A%7B%22value%22%3A%22globalrelevanceex%22%7D%7D%2C%22isListVisible%22%3Atrue%2C%22mapZoom%22%3A6%7D\"\n",
    "\n",
    "# Define headers for the HTTP request\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    \"Referer\": \"https://www.zillow.com/\"\n",
    "}\n",
    "\n",
    "# Function to introduce random delays\n",
    "def random_delay(min_delay=0.2, max_delay=0.5):\n",
    "    delay = random.uniform(min_delay, max_delay)\n",
    "    print(f\"Sleeping for {delay:.2f} seconds...\")\n",
    "    time.sleep(delay)\n",
    "\n",
    "# list to store extracted data\n",
    "data = []\n",
    "\n",
    "# Loop to scrape multiple pages\n",
    "page_number = 1  # Starting page number\n",
    "while True:\n",
    "    # Construct the URL for the current page\n",
    "    url = base_url.format(page_number=page_number)\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        print(f\"Successfully retrieved page {page_number}\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve page {page_number}. Status code: {response.status_code}\")\n",
    "        break \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the property cards where class contains \"StyledPropertyCardDataWrapper\"\n",
    "    property_cards = soup.find_all(\"div\", class_=re.compile(\".*StyledPropertyCardDataWrapper.*\"))\n",
    "\n",
    "    if not property_cards:  # If no property cards are found, break the loop\n",
    "        print(\"No property cards found. Stopping scraping.\")\n",
    "        break\n",
    "\n",
    "    # Extract details for each property card\n",
    "    for idx, card in enumerate(property_cards, start=1):\n",
    "        try:\n",
    "            # Extract property link\n",
    "            link = card.find(\"a\", class_=\"StyledPropertyCardDataArea-c11n-8-107-0__sc-10i1r6-0\")['href']\n",
    "            full_link = f\"https://www.zillow.com{link}\" if link.startswith('/') else link\n",
    "            \n",
    "            # Extract address\n",
    "            address = card.find(\"address\", {\"data-test\": \"property-card-addr\"}).get_text(strip=True)\n",
    "            \n",
    "            # Extract price\n",
    "            price = card.find(\"span\", {\"data-test\": \"property-card-price\"}).get_text(strip=True)\n",
    "            \n",
    "            # Extract bedrooms, bathrooms, and size\n",
    "            details = card.find(\"ul\", class_=\"StyledPropertyCardHomeDetailsList-c11n-8-107-0__sc-1j0som5-0\")\n",
    "            bds, ba, sqft = [li.get_text(strip=True) for li in details.find_all(\"li\")[:3]]\n",
    "            \n",
    "            # Append the extracted data\n",
    "            data.append({\n",
    "                \"Address\": address,\n",
    "                \"Price\": price,\n",
    "                \"Bedrooms\": bds,\n",
    "                \"Bathrooms\": ba,\n",
    "                \"Size\": sqft,\n",
    "                \"Link\": full_link\n",
    "            })\n",
    "            print(f\"Extracted property {idx + (page_number-1)*20}: {address}\")\n",
    "            \n",
    "            # Introduce a delay between each property extraction\n",
    "            random_delay()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting data for property {idx}: {e}\")\n",
    "\n",
    "    # Find the next page link by searching for the \"Next\" button in the <li> element\n",
    "    next_page_li = soup.find(\"li\", class_=\"PaginationJumpItem-c11n-8-107-0__sc-h97wcm-0 kdGqlo\")\n",
    "    next_page_a = next_page_li.find(\"a\", rel=\"next\") if next_page_li else None\n",
    "\n",
    "    if next_page_a:\n",
    "        # Extract the next page URL and update the page number\n",
    "        next_page_url = next_page_a['href']\n",
    "        page_number = int(re.search(r'(\\d+)_p', next_page_url).group(1))\n",
    "        print(f\"Moving to next page: {page_number}\")\n",
    "    else:\n",
    "        # If no next page link is found, break the loop\n",
    "        print(\"No next page found. Stopping scraping.\")\n",
    "        break\n",
    "\n",
    "# Convert to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"zillow_properties_with_delays.csv\", index=False)\n",
    "print(\"Data successfully saved to 'zillow_properties_with_delays.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936b5a15-dcde-43d2-8615-303f0bdf8cab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
